{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67dbfd8",
   "metadata": {},
   "source": [
    "# Openminded Pysyft 0.29 MNIST using a CNN code\n",
    "# Federated Learning Course Gharibim code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f15011",
   "metadata": {},
   "source": [
    "# 필요한 라이브러리 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3964750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import syft as sy\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "from ipynb.fs.full.FLDataset import load_dataset, getImgs, getData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460aec3",
   "metadata": {},
   "source": [
    "# 사용할 인자 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5129c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.images = 60000 #총 이미지 수\n",
    "        self.clients = 10 #총 클라이언트 수\n",
    "        self.rounds = 5 #모든 클라이언트가 한번씩 학습 한 결과를 1라운드\n",
    "        self.epochs = 5 #훈련 데이터 전체를 반복하는 횟수\n",
    "        self.local_batches = 64 #학습 한번에 불러올 데이터 크기\n",
    "        self.lr = 0.01 #얼마나 빠르게 학습할 것인가?\n",
    "        self.C = 0.9 #라운드에 얼마나 많은 클라이언트를 사용할 것인가\n",
    "        self.drop_rate = 0.1\n",
    "        self.torch_seed = 0 #랜덤 시드 고정\n",
    "        self.log_interval = 100 #10번의 에포크 마다 학습 결과를 출력하기 위한 인자\n",
    "        self.iid = 'iid' #iid환경에서의 테스트를 하기 위한 인자\n",
    "        self.split_size = int(self.images / self.clients) #60000/client 수 즉, 클라이언트 마다 할당되는 데이터의 수 \n",
    "        self.samples = self.split_size / self.images #논문에서 정의하고 있는 샘플 크기 (nk/n)\n",
    "        self.use_cuda = False\n",
    "        self.save_model = False\n",
    "\n",
    "#args라는 변수에 모두 저장되어있어 이후 코드에서 사용할 수 있음\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = args.use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ddeb6",
   "metadata": {},
   "source": [
    "# 클라이언트 생성(워커)\n",
    "* 클라이언트들이 포함된 배열을 만들 것인데, 그 안에 포함되는 값들을 사전 구조로 묶어줄 것이다. \n",
    "* key값은 hook이라는 것을 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df441b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'fl': <VirtualWorker id:client1 #objects:0>}, {'fl': <VirtualWorker id:client2 #objects:0>}, {'fl': <VirtualWorker id:client3 #objects:0>}, {'fl': <VirtualWorker id:client4 #objects:0>}, {'fl': <VirtualWorker id:client5 #objects:0>}, {'fl': <VirtualWorker id:client6 #objects:0>}, {'fl': <VirtualWorker id:client7 #objects:0>}, {'fl': <VirtualWorker id:client8 #objects:0>}, {'fl': <VirtualWorker id:client9 #objects:0>}, {'fl': <VirtualWorker id:client10 #objects:0>}]\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "clients = []\n",
    "\n",
    "#10개의 사전 데이터 형태를 생성하고 value에 VritualWorkder id: 값 client+i 를 생성한다.\n",
    "for i in range(args.clients):\n",
    "    clients.append({'fl': sy.virtualWorker(hook, id=\"client{}\".format(i+1))}) \n",
    "print(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d03181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-19 09:21:21--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "www.di.ens.fr (www.di.ens.fr)을(를) 해석하는 중... 129.199.99.14\n",
      "접속 www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... 접속됨.\n",
      "HTTP 요청을 전송했습니다. 응답을 기다리는 중입니다... 302 Found\n",
      "위치: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [따라감]\n",
      "--2021-07-19 09:21:22--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "접속 www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... 접속됨.\n",
      "HTTP 요청을 전송했습니다. 응답을 기다리는 중입니다... 200 OK\n",
      "길이: 지정되지 않음 [application/x-gzip]\n",
      "다음 위치에 저장: `MNIST.tar.gz.1'\n",
      "\n",
      "MNIST.tar.gz.1          [            <=>     ]  17.17M  1.49MB/s               ^C\n",
      "MNIST/\n",
      "MNIST/raw/\n",
      "MNIST/raw/train-labels-idx1-ubyte\n",
      "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "MNIST/raw/t10k-labels-idx1-ubyte\n",
      "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "MNIST/raw/train-images-idx3-ubyte\n",
      "MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "MNIST/raw/t10k-images-idx3-ubyte\n",
      "MNIST/raw/train-images-idx3-ubyte.gz\n",
      "MNIST/processed/\n",
      "MNIST/processed/training.pt\n",
      "MNIST/processed/test.pt\n"
     ]
    }
   ],
   "source": [
    "# 다운로드 MNIST\n",
    "#!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "#!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1950650f",
   "metadata": {},
   "source": [
    "앞서 생성한 mnist iid 데이터 분할 방법을 사용하여 데이터를 global_train, global_test, train_goup, test_group으로 나눈다.\n",
    "이후 train_group과 test_group을 활용하여 클라이언트 별 훈련 데이터, 테스트 데이터를 나눠놓는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9adb5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_train, global_test, train_group, test_group = load_dataset(args.clients, args.iid)\n",
    "# 클라이언트 별로 데이터로더를 생성\n",
    "for idx, client in enumerate(clients):\n",
    "    trainset_idx_list = list(train_group[idx])\n",
    "    client['trainset'] = getImgs(global_train, trainset_idx_list, args.local_batches) # 훈련 데이터 로더\n",
    "    client['testset'] = getImgs(global_test, list(test_group[idx]), args.local_batches) #테스트 데이터 로더\n",
    "    client['samples'] = len(trainset_idx_list) / args.images #추후 사용할 samples변수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21d0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getImgs하기 전 데이터 모양은 데이터셋 전체이기 때문에, 데이터 로더를 사용하여 전체 테스트 데이터셋 데이터 로더를 생성\n",
    "global_test_loader = DataLoader(global_test, batch_size=args.local_batches, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f06004",
   "metadata": {},
   "source": [
    "# 학습하려는 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916c4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) #input, output, kernel_size, stride\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        #[-1, 50, 8,8]\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # x shape[-1, 20, 24, 24]\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        # x shape[-1, 20, 12, 12] \n",
    "        x = F.relu(self.conv2(x))\n",
    "        # x shape[-1, 50, 8, 8]\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        # x shape[-1, 50, 4, 4]\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x shape [-1, 500]\n",
    "        x = self.fc2(x)\n",
    "        #x shape [-1, 10]\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7259731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClientUpdate(args, device, client):\n",
    "    client['model'].train()\n",
    "    client['model'].send(client['fl'])\n",
    "    \n",
    "    #에포크 수만큼 반복 시키기 위해 범위를 지정했고,\n",
    "    #출력문 문제 때문에 1부터 시작하였음\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for batch_idx, (X, y) in enumerate(client['trainset']):\n",
    "            #X와 Y를 불러오고 X와 y를 클라이언트에게 전송함\n",
    "            X = X.send(client['fl'])\n",
    "            y = y.send(client['fl'])\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            #학습 프로세스\n",
    "            client['optim'].zero_grad() # 그라디언트 초기화\n",
    "            output = client['model'](X) # 모델의 예측값 획득\n",
    "            loss = F.nll_loss(output, y) # loss 계산\n",
    "            loss.backward() #역전파\n",
    "            client['optim'].step() #파라미터 업데이트\n",
    "            \n",
    "            #loss를 출력하기 위한 출력문\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                loss = loss.get() \n",
    "                print('Model {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    client['fl'].id,\n",
    "                    epoch, batch_idx * args.local_batches, len(client['trainset']) * args.local_batches, \n",
    "                    100. * batch_idx / len(client['trainset']), loss))\n",
    "                \n",
    "    client['model'].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e320ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader, name):\n",
    "    model.eval()   \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        #test 데이터 로더를 불러와서 예측해보기\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X)\n",
    "            test_loss += F.nll_loss(output, y, reduction='sum').item() #배치 로스 합\n",
    "            pred = output.argmax(1, keepdim=True) # 결과 값으로 출력되는 log-probability를 클래스 숫자로 변경 [0,0,0,1,0,0,0]\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss for {} model: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        name, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c07e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageModels(global_model, clients):\n",
    "    client_models = [clients[i]['model'] for i in range(len(clients))]\n",
    "    samples = [clients[i]['samples'] for i in range(len(clients))]\n",
    "    global_dict = global_model.state_dict()\n",
    "    \n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() * samples[i] for i in range(len(client_models))], 0).sum(0)\n",
    "            \n",
    "    global_model.load_state_dict(global_dict)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ff3b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dilab/anaconda3/envs/pysyft/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook.py:560: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_tensor = hook_self.torch.native_tensor(*args, **kwargs)\n",
      "/home/dilab/anaconda3/envs/pysyft/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py:156: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  to_return = self.native_grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model client3 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.304334\n",
      "Model client3 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.264463\n",
      "Model client3 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.124718\n",
      "Model client3 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.623853\n",
      "Model client3 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.717422\n",
      "Model client9 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.302658\n",
      "Model client9 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.266213\n",
      "Model client9 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.177051\n",
      "Model client9 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.716066\n",
      "Model client9 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.922652\n",
      "Model client5 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.307335\n",
      "Model client5 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.266932\n",
      "Model client5 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.128643\n",
      "Model client5 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.619649\n",
      "Model client5 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.693400\n",
      "Model client10 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.297128\n",
      "Model client10 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.268785\n",
      "Model client10 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.159676\n",
      "Model client10 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.566709\n",
      "Model client10 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.809650\n",
      "Model client2 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.310549\n",
      "Model client2 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.250687\n",
      "Model client2 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.144503\n",
      "Model client2 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.620666\n",
      "Model client2 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.672178\n",
      "Model client7 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.306483\n",
      "Model client7 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.262219\n",
      "Model client7 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.165448\n",
      "Model client7 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.576966\n",
      "Model client7 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.735273\n",
      "Model client8 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.303251\n",
      "Model client8 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.256979\n",
      "Model client8 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.107229\n",
      "Model client8 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.583199\n",
      "Model client8 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.674267\n",
      "Model client4 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.313309\n",
      "Model client4 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.267294\n",
      "Model client4 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.115337\n",
      "Model client4 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.567134\n",
      "Model client4 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.936612\n",
      "Model client1 Train Epoch: 1 [0/6016 (0%)]\tLoss: 2.305843\n",
      "Model client1 Train Epoch: 2 [0/6016 (0%)]\tLoss: 2.265807\n",
      "Model client1 Train Epoch: 3 [0/6016 (0%)]\tLoss: 2.153233\n",
      "Model client1 Train Epoch: 4 [0/6016 (0%)]\tLoss: 1.668428\n",
      "Model client1 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.841567\n",
      "\n",
      "Test set: Average loss for Global model: 0.6817, Accuracy: 8611/10000 (86%)\n",
      "\n",
      "Model client3 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.662571\n",
      "Model client3 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.404639\n",
      "Model client3 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.405498\n",
      "Model client3 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.232197\n",
      "Model client3 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.406061\n",
      "Model client10 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.667264\n",
      "Model client10 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.404417\n",
      "Model client10 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.570575\n",
      "Model client10 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.680401\n",
      "Model client10 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.458169\n",
      "Model client7 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.840815\n",
      "Model client7 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.364236\n",
      "Model client7 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.326871\n",
      "Model client7 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.372118\n",
      "Model client7 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.636399\n",
      "Model client5 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.820970\n",
      "Model client5 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.667459\n",
      "Model client5 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.337330\n",
      "Model client5 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.515229\n",
      "Model client5 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.597752\n",
      "Model client1 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.788310\n",
      "Model client1 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.501783\n",
      "Model client1 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.495477\n",
      "Model client1 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.276344\n",
      "Model client1 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.394673\n",
      "Model client4 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.762435\n",
      "Model client4 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.729331\n",
      "Model client4 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.343985\n",
      "Model client4 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.557987\n",
      "Model client4 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.299078\n",
      "Model client2 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.639294\n",
      "Model client2 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.526987\n",
      "Model client2 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.310177\n",
      "Model client2 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.424058\n",
      "Model client2 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.417108\n",
      "Model client8 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.668739\n",
      "Model client8 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.315835\n",
      "Model client8 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.204836\n",
      "Model client8 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.585441\n",
      "Model client8 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.355333\n",
      "Model client9 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.575912\n",
      "Model client9 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.391730\n",
      "Model client9 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.346483\n",
      "Model client9 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.307847\n",
      "Model client9 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.283462\n",
      "\n",
      "Test set: Average loss for Global model: 0.3814, Accuracy: 9142/10000 (91%)\n",
      "\n",
      "Model client5 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.278426\n",
      "Model client5 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.263265\n",
      "Model client5 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.257920\n",
      "Model client5 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.309368\n",
      "Model client5 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.247096\n",
      "Model client2 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.331582\n",
      "Model client2 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.341364\n",
      "Model client2 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.258142\n",
      "Model client2 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.234534\n",
      "Model client2 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.340622\n",
      "Model client6 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.311438\n",
      "Model client6 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.379983\n",
      "Model client6 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.494522\n",
      "Model client6 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.233686\n",
      "Model client6 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.218554\n",
      "Model client1 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.365042\n",
      "Model client1 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.399605\n",
      "Model client1 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.289850\n",
      "Model client1 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.223816\n",
      "Model client1 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.333869\n",
      "Model client8 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.434655\n",
      "Model client8 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.465478\n",
      "Model client8 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.349471\n",
      "Model client8 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.195951\n",
      "Model client8 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.384757\n",
      "Model client3 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.448860\n",
      "Model client3 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.284667\n",
      "Model client3 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.261549\n",
      "Model client3 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.165355\n",
      "Model client3 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.154495\n",
      "Model client4 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.382129\n",
      "Model client4 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.509889\n",
      "Model client4 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.278946\n",
      "Model client4 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.304918\n",
      "Model client4 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.225762\n",
      "Model client7 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.477860\n",
      "Model client7 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.394945\n",
      "Model client7 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.216688\n",
      "Model client7 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.350839\n",
      "Model client7 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.315814\n",
      "Model client10 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.419036\n",
      "Model client10 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.345025\n",
      "Model client10 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.353811\n",
      "Model client10 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.109162\n",
      "Model client10 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.227011\n",
      "\n",
      "Test set: Average loss for Global model: 0.3097, Accuracy: 9298/10000 (93%)\n",
      "\n",
      "Model client6 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.244910\n",
      "Model client6 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.261863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model client6 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.160188\n",
      "Model client6 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.178211\n",
      "Model client6 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.184034\n",
      "Model client5 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.313504\n",
      "Model client5 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.270600\n",
      "Model client5 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.235557\n",
      "Model client5 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.178778\n",
      "Model client5 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.228301\n",
      "Model client2 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.223734\n",
      "Model client2 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.351724\n",
      "Model client2 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.231041\n",
      "Model client2 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.302091\n",
      "Model client2 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.183283\n",
      "Model client3 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.316389\n",
      "Model client3 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.216821\n",
      "Model client3 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.250107\n",
      "Model client3 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.225035\n",
      "Model client3 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.259652\n",
      "Model client10 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.278715\n",
      "Model client10 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.253158\n",
      "Model client10 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.207820\n",
      "Model client10 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.190390\n",
      "Model client10 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.171830\n",
      "Model client7 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.448383\n",
      "Model client7 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.300083\n",
      "Model client7 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.198159\n",
      "Model client7 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.257450\n",
      "Model client7 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.186813\n",
      "Model client8 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.157533\n",
      "Model client8 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.300089\n",
      "Model client8 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.218077\n",
      "Model client8 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.213130\n",
      "Model client8 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.177311\n",
      "Model client1 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.298879\n",
      "Model client1 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.247285\n",
      "Model client1 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.263353\n",
      "Model client1 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.186340\n",
      "Model client1 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.134662\n",
      "Model client4 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.319857\n",
      "Model client4 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.144907\n",
      "Model client4 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.168420\n",
      "Model client4 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.332991\n",
      "Model client4 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.212189\n",
      "\n",
      "Test set: Average loss for Global model: 0.2659, Accuracy: 9424/10000 (94%)\n",
      "\n",
      "Model client4 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.380248\n",
      "Model client4 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.251016\n",
      "Model client4 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.379114\n",
      "Model client4 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.251577\n",
      "Model client4 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.145352\n",
      "Model client9 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.301854\n",
      "Model client9 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.205439\n",
      "Model client9 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.198101\n",
      "Model client9 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.348075\n",
      "Model client9 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.206871\n",
      "Model client5 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.235593\n",
      "Model client5 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.234907\n",
      "Model client5 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.305271\n",
      "Model client5 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.197625\n",
      "Model client5 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.205984\n",
      "Model client10 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.374001\n",
      "Model client10 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.164308\n",
      "Model client10 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.214537\n",
      "Model client10 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.314256\n",
      "Model client10 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.191818\n",
      "Model client3 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.268765\n",
      "Model client3 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.366681\n",
      "Model client3 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.356736\n",
      "Model client3 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.107861\n",
      "Model client3 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.211075\n",
      "Model client7 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.278418\n",
      "Model client7 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.292002\n",
      "Model client7 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.104510\n",
      "Model client7 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.389661\n",
      "Model client7 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.133704\n",
      "Model client1 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.344218\n",
      "Model client1 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.205627\n",
      "Model client1 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.117952\n",
      "Model client1 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.208780\n",
      "Model client1 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.291563\n",
      "Model client2 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.281945\n",
      "Model client2 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.237812\n",
      "Model client2 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.354386\n",
      "Model client2 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.145318\n",
      "Model client2 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.184329\n",
      "Model client6 Train Epoch: 1 [0/6016 (0%)]\tLoss: 0.271840\n",
      "Model client6 Train Epoch: 2 [0/6016 (0%)]\tLoss: 0.247864\n",
      "Model client6 Train Epoch: 3 [0/6016 (0%)]\tLoss: 0.291460\n",
      "Model client6 Train Epoch: 4 [0/6016 (0%)]\tLoss: 0.390597\n",
      "Model client6 Train Epoch: 5 [0/6016 (0%)]\tLoss: 0.181797\n",
      "\n",
      "Test set: Average loss for Global model: 0.2385, Accuracy: 9503/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.torch_seed) # innitialize w0\n",
    "global_model = Net() #initialize model\n",
    "\n",
    "#로컬 클라이언트 모델을 torch cpu에 로드 시키고 최적화함수를 해당 클라이언트 모델로 갱신하는 방식\n",
    "#clients라는 사전에 모두 저장할 수 있도록 코드를 구성함\n",
    "for client in clients:\n",
    "    torch.manual_seed(args.torch_seed) \n",
    "    client['model'] = Net().to(device)\n",
    "    client['optim'] = optim.SGD(client['model'].parameters(), lr=args.lr)\n",
    "\n",
    "for fed_round in range(args.rounds):\n",
    "    \n",
    "    # number of selected clients\n",
    "    m = int(max(args.C * args.clients, 1))\n",
    "\n",
    "    # 선택된 클라이언트 집합을 생성하는 방법\n",
    "    np.random.seed(fed_round)\n",
    "    selected_clients_inds = np.random.choice(range(len(clients)), m, replace=False) #10개의 클라이언트 중 9개의 클라이언트를 선택함\n",
    "    selected_clients = [clients[i] for i in selected_clients_inds]\n",
    "\n",
    "    \n",
    "    # 학습 진행\n",
    "    # 논문에서 학습 진행에 ClientUpdate함수가 사용되기때문에 이를 구현\n",
    "    for client in selected_clients:\n",
    "        ClientUpdate(args, device, client)\n",
    "    \n",
    "    # 평균\n",
    "    global_model = averageModels(global_model, selected_clients)\n",
    "    \n",
    "    # Testing the average model\n",
    "    test(args, global_model, device, global_test_loader, 'Global')\n",
    "            \n",
    "    # Share the global model with the clients\n",
    "    for client in clients:\n",
    "        client['model'].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "if (args.save_model):\n",
    "    torch.save(global_model.state_dict(), \"FedAvg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6dbc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de0e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysyft",
   "language": "python",
   "name": "pysyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
